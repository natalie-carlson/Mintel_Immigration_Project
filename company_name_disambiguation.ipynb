{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mintel Database Company Name Disambiguation\n",
    "This notebook attempts to standardize company names from the Mintel Product Database with a fuzzy matching strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Ultimate Company                    Company  \\\n",
      "1                          22                         22   \n",
      "2                          30                         30   \n",
      "3                          31                         31   \n",
      "4                         899                        899   \n",
      "5                         999                        999   \n",
      "6                         999                        999   \n",
      "7                        1701                       1701   \n",
      "8                        1907                       1907   \n",
      "9                        2376                       2376   \n",
      "10                     232605                     232605   \n",
      "11   Hong Guan Kamtai (China)   Hong Guan Kamtai (China)   \n",
      "12           Êxito Naturavene           Êxito Naturavene   \n",
      "13                     Ñaming                     Ñaming   \n",
      "14                     Ñapi-ú                     Ñapi-ú   \n",
      "\n",
      "                                      Company address Company county/State  \\\n",
      "1      40 Vu Xuan Thieu, Sai Dong, Long Bien, Hanoi,                   NaN   \n",
      "2                                                 NaN                  NaN   \n",
      "3                                                 NaN                  NaN   \n",
      "4                                                 NaN                  NaN   \n",
      "5                                                 NaN                  NaN   \n",
      "6                                                 NaN                  NaN   \n",
      "7                                                 NaN                  NaN   \n",
      "8                                                 NaN                  NaN   \n",
      "9                                                 NaN                  NaN   \n",
      "10         PO Box 1025, Mississauga, Ontario, L4Y 3W3              Ontario   \n",
      "11                                                NaN                  NaN   \n",
      "12         Rua Prof. Miguel Russiano 320, Sao Paulo,                   NaN   \n",
      "13  Pol. Ind. El Zafranar, Parcelas 21.1 y 21.2, M...                  NaN   \n",
      "14      Tucumán 845, Corrientes, Bella Vista, W342CYQ                  NaN   \n",
      "\n",
      "   Company Territory         Website  \n",
      "1            Vietnam             NaN  \n",
      "2             Poland             NaN  \n",
      "3          Indonesia             NaN  \n",
      "4          Indonesia             NaN  \n",
      "5              China             NaN  \n",
      "6          Indonesia             NaN  \n",
      "7       South Africa  www.1701.co.za  \n",
      "8        New Zealand             NaN  \n",
      "9          Indonesia             NaN  \n",
      "10            Canada             NaN  \n",
      "11               NaN             NaN  \n",
      "12            Brazil             NaN  \n",
      "13             Spain  www.n-aming.es  \n",
      "14         Argentina   www.napiu.com  \n"
     ]
    }
   ],
   "source": [
    "# read in company data\n",
    "companies = pd.read_csv('/Users/ncarlson/Dropbox/Mintel Immigration Project/GNPD-2009-19.csv', encoding = \"cp1252\")\n",
    "# print head of file\n",
    "print(companies[1:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197175"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique companies\n",
    "len(pd.unique(companies['Ultimate Company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller subset of data to test code\n",
    "companies_sample = companies.sample(n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Processing\n",
    "Next, we'll clean and preprocess the company names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing special characters\n",
    "def clean_special_characters(txt):\n",
    "    seps = [' ','`','~','!','@','#','$','%','^','&','*','(',')','_','-','+','=','{','[','}','}','|',':',';','<',',','>','.','?','/']\n",
    "    default_sep = seps[0]\n",
    "    \n",
    "    for sep in seps[1:]:\n",
    "            txt = txt.replace(sep, default_sep)\n",
    "    re.sub(' +', ' ', txt)\n",
    "    temp_list = [i.strip() for i in txt.split(default_sep)]\n",
    "    temp_list = [i for i in temp_list if i]\n",
    "    return ' '.join(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# function for removing stopwords -- using nltk english default for now\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "my_stopwords = stopwords.words('english')\n",
    "\n",
    "def clean_stopwords(txt):\n",
    "    temp_list = txt.split(' ')\n",
    "    temp_list = [i for i in temp_list if i not in my_stopwords]\n",
    "    return ' '.join(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning function -- putting the previous two together \n",
    "def data_cleaning(data, nameCol='Ultimate Company'):\n",
    "    data.dropna(subset=[nameCol], inplace=True)\n",
    "    data = data.rename_axis('CompanyID').reset_index()\n",
    "    data_clean = data.copy()\n",
    "    data_clean['CompanyName_clean'] = data_clean[nameCol].apply(lambda x: x.lower())\n",
    "    data_clean['CompanyName_clean'] = data_clean['CompanyName_clean'].apply(clean_special_characters)\n",
    "    data_clean['CompanyName_clean'] = data_clean['CompanyName_clean'].apply(clean_stopwords)\n",
    "    data_clean = data_clean.sort_values(by=['CompanyName_clean'])\n",
    "    return(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "9995\n"
     ]
    }
   ],
   "source": [
    "# run data cleaning function on our data\n",
    "companies_clean = data_cleaning(companies_sample, nameCol='Ultimate Company')\n",
    "print(len(companies_clean))\n",
    "# remove empty company names\n",
    "companies_clean = companies_clean[companies_clean['CompanyName_clean']!='']\n",
    "print(len(companies_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching\n",
    "Next, we compute distance using the FuzzyWuzzy library. It's intractable to compute pairwise distance between 200,000^2 matches so we'll group by first character(s) and compute within. We will test out different metrics -- e.g. Levenschtein ratio, partial ratio, etc. -- and different cutoffs to choose the optimal approach. See this guide for more details on the specific ratios: https://www.datacamp.com/community/tutorials/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy matching by group \n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "# Levenschtein ratio\n",
    "all_main_name = pd.DataFrame(columns=['first_characters','names','alias','score'])\n",
    "all_names = companies_clean['CompanyName_clean'].unique()\n",
    "all_main_name['names'] = all_names\n",
    "all_main_name['first_characters'] = all_main_name['names'].apply(lambda x: x[0])\n",
    "all_sort_gp = all_main_name['first_characters'].unique()\n",
    "\n",
    "for sortgp in all_sort_gp:\n",
    "    this_gp = all_main_name.groupby(['first_characters']).get_group(sortgp)\n",
    "    gp_start = this_gp.index.min()\n",
    "    gp_end = this_gp.index.max()\n",
    "    for i in range(gp_start,gp_end+1):\n",
    "    \n",
    "        # if self has not got alias, asign to be alias of itself\n",
    "        if pd.isna(all_main_name['alias'].iloc[i]):\n",
    "            all_main_name['alias'].iloc[i] = all_main_name['names'].iloc[i]\n",
    "            all_main_name['score'].iloc[i] = 100\n",
    "        \n",
    "        # if the following has not got alias and fuzzy match, asign to be alias of this one\n",
    "        for j in range(i+1,gp_end+1):\n",
    "            if pd.isna(all_main_name['alias'].iloc[j]):\n",
    "                fuzz_score = fuzz.ratio(all_main_name['names'].iloc[i],all_main_name['names'].iloc[j])\n",
    "                if (fuzz_score > 85):\n",
    "                    all_main_name['alias'].iloc[j] = all_main_name['alias'].iloc[i]\n",
    "                    all_main_name['score'].iloc[j] = fuzz_score\n",
    "\n",
    "all_main_name.to_csv('/Users/ncarlson/Dropbox/Mintel Immigration Project/Levenschtein.csv')\n",
    "\n",
    "# Partial ratio\n",
    "all_main_name = pd.DataFrame(columns=['first_characters','names','alias','score'])\n",
    "all_names = companies_clean['CompanyName_clean'].unique()\n",
    "all_main_name['names'] = all_names\n",
    "all_main_name['first_characters'] = all_main_name['names'].apply(lambda x: x[0])\n",
    "all_sort_gp = all_main_name['first_characters'].unique()\n",
    "\n",
    "for sortgp in all_sort_gp:\n",
    "    this_gp = all_main_name.groupby(['first_characters']).get_group(sortgp)\n",
    "    gp_start = this_gp.index.min()\n",
    "    gp_end = this_gp.index.max()\n",
    "    for i in range(gp_start,gp_end+1):\n",
    "    \n",
    "        # if self has not got alias, asign to be alias of itself\n",
    "        if pd.isna(all_main_name['alias'].iloc[i]):\n",
    "            all_main_name['alias'].iloc[i] = all_main_name['names'].iloc[i]\n",
    "            all_main_name['score'].iloc[i] = 100\n",
    "        \n",
    "        # if the following has not got alias and fuzzy match, asign to be alias of this one\n",
    "        for j in range(i+1,gp_end+1):\n",
    "            if pd.isna(all_main_name['alias'].iloc[j]):\n",
    "                fuzz_score = fuzz.partial_ratio(all_main_name['names'].iloc[i],all_main_name['names'].iloc[j])\n",
    "                if (fuzz_score > 85):\n",
    "                    all_main_name['alias'].iloc[j] = all_main_name['alias'].iloc[i]\n",
    "                    all_main_name['score'].iloc[j] = fuzz_score\n",
    "\n",
    "all_main_name.to_csv('/Users/ncarlson/Dropbox/Mintel Immigration Project/Partial.csv')\n",
    "\n",
    "\n",
    "# harmonic mean of token set ratio and partial ratio\n",
    "all_main_name = pd.DataFrame(columns=['first_characters','names','alias','score'])\n",
    "all_names = companies_clean['CompanyName_clean'].unique()\n",
    "all_main_name['names'] = all_names\n",
    "all_main_name['first_characters'] = all_main_name['names'].apply(lambda x: x[0])\n",
    "all_sort_gp = all_main_name['first_characters'].unique()\n",
    "\n",
    "for sortgp in all_sort_gp:\n",
    "    this_gp = all_main_name.groupby(['first_characters']).get_group(sortgp)\n",
    "    gp_start = this_gp.index.min()\n",
    "    gp_end = this_gp.index.max()\n",
    "    for i in range(gp_start,gp_end+1):\n",
    "    \n",
    "        # if self has not got alias, asign to be alias of itself\n",
    "        if pd.isna(all_main_name['alias'].iloc[i]):\n",
    "            all_main_name['alias'].iloc[i] = all_main_name['names'].iloc[i]\n",
    "            all_main_name['score'].iloc[i] = 100\n",
    "        \n",
    "        # if the following has not got alias and fuzzy match, asign to be alias of this one\n",
    "        for j in range(i+1,gp_end+1):\n",
    "            if pd.isna(all_main_name['alias'].iloc[j]):\n",
    "                s1 = fuzz.partial_ratio(all_main_name['names'].iloc[i],all_main_name['names'].iloc[j])\n",
    "                s2 = fuzz.token_set_ratio(all_main_name['names'].iloc[i],all_main_name['names'].iloc[j])\n",
    "                fuzz_score = 2*s1*s2 / (s1 + s2)\n",
    "                if (fuzz_score > 85):\n",
    "                    all_main_name['alias'].iloc[j] = all_main_name['alias'].iloc[i]\n",
    "                    all_main_name['score'].iloc[j] = fuzz_score\n",
    "\n",
    "all_main_name.to_csv('/Users/ncarlson/Dropbox/Mintel Immigration Project/Harmonic_Mean.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
